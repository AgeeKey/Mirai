#!/usr/bin/env python3
"""
üåê WEB SCRAPER AGENT - –†–µ–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
================================================================

–ê–≥–µ–Ω—Ç –¥–ª—è –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç–∞:
- –ü–æ–∏—Å–∫ –≤ Google —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ HTML —Å—Ç—Ä–∞–Ω–∏—Ü
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- AI –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –†–µ–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞

–ê–≤—Ç–æ—Ä: MIRAI Team
–î–∞—Ç–∞: 2025-10-25
"""

import asyncio
import logging
import re
from typing import Any, Dict, List, Optional
from urllib.parse import quote_plus, urlparse

import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class WebScraperAgent:
    """
    –ê–≥–µ–Ω—Ç –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü.
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ü–æ–∏—Å–∫ –≤ Google
    - –ü–∞—Ä—Å–∏–Ω–≥ HTML
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    - –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ AI
    """
    
    def __init__(self, ai_manager=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞.
        
        Args:
            ai_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä AI –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        """
        self.ai = ai_manager
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        })
        logger.info("‚úÖ WebScraperAgent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def search_and_analyze(
        self,
        query: str,
        num_results: int = 3,
        analyze: bool = True
    ) -> Dict[str, Any]:
        """
        –ü–æ–∏—Å–∫ –≤ Google –∏ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            num_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–π—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analyze: –ü—Ä–æ–≤–æ–¥–∏—Ç—å AI –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        logger.info(f"üîç –ü–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑: {query}")
        
        # 1. –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ –≤ Google
        search_results = await self._google_search(query)
        
        if not search_results:
            return {
                'success': False,
                'query': query,
                'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞'
            }
        
        # 2. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø–∞—Ä—Å–∏—Ç—å —Ç–æ–ø-N —Å–∞–π—Ç–æ–≤
        content_list = []
        for i, result in enumerate(search_results[:num_results]):
            logger.info(f"üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∞–π—Ç–∞ {i+1}/{num_results}: {result.get('title', 'N/A')}")
            
            content = await self._scrape_page(result['url'])
            if content:
                content_list.append({
                    'url': result['url'],
                    'title': result['title'],
                    'snippet': result.get('snippet', ''),
                    'content': content
                })
        
        # 3. AI –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        analysis = None
        if analyze and self.ai and content_list:
            logger.info("üß† AI –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
            analysis = await self._analyze_content(query, content_list)
        
        return {
            'success': True,
            'query': query,
            'search_results': search_results,
            'scraped_content': content_list,
            'analysis': analysis,
            'summary': {
                'total_results': len(search_results),
                'scraped_pages': len(content_list),
                'analyzed': analysis is not None
            }
        }
    
    async def _google_search(self, query: str) -> List[Dict[str, str]]:
        """
        –ü–æ–∏—Å–∫ –≤ Google –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å URL, –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å–Ω–∏–ø–ø–µ—Ç–∞–º–∏
        """
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –ø–æ–∏—Å–∫–∞
            encoded_query = quote_plus(query)
            search_url = f"https://www.google.com/search?q={encoded_query}&hl=ru"
            
            logger.info(f"üîç Google –ø–æ–∏—Å–∫: {search_url}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            
            # –ò—â–µ–º –±–ª–æ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (Google —á–∞—Å—Ç–æ –º–µ–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
            for g in soup.select('div.g, div[data-sokoban-container]'):
                try:
                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = g.select_one('h3')
                    if not title_elem:
                        continue
                    title = title_elem.get_text(strip=True)
                    
                    # –°—Å—ã–ª–∫–∞
                    link_elem = g.select_one('a')
                    if not link_elem or not link_elem.get('href'):
                        continue
                    url = link_elem['href']
                    
                    # –£–±–∏—Ä–∞–µ–º Google —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
                    if url.startswith('/url?q='):
                        url = url.split('/url?q=')[1].split('&')[0]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL
                    if not url.startswith('http'):
                        continue
                    
                    # –°–Ω–∏–ø–ø–µ—Ç (–æ–ø–∏—Å–∞–Ω–∏–µ)
                    snippet_elem = g.select_one('div[data-sncf], div.VwiC3b, span.aCOpRe')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                    
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet
                    })
                    
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                    continue
            
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Google: {e}")
            return []
    
    async def _scrape_page(self, url: str, max_length: int = 10000) -> Optional[str]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ None
        """
        try:
            logger.debug(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞: {url}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = self.session.get(url, timeout=10, allow_redirects=True)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏, –Ω–∞–≤–∏–≥–∞—Ü–∏—é
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
                element.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            main_content = soup.select_one('article, main, div.content, div.post')
            if main_content:
                text = main_content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            lines = [line.strip() for line in text.splitlines()]
            # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ (–º–µ–Ω—é, –∫–Ω–æ–ø–∫–∏ –∏ —Ç.–¥.)
            lines = [line for line in lines if line and len(line) > 20]
            text = '\n'.join(lines)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            if len(text) > max_length:
                text = text[:max_length]
            
            logger.debug(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {url}")
            return text
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}: {e}")
            return None
    
    async def _analyze_content(
        self,
        query: str,
        content_list: List[Dict[str, str]]
    ) -> Optional[str]:
        """
        –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ AI.
        
        Args:
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            content_list: –°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            
        Returns:
            AI –∞–Ω–∞–ª–∏–∑ –∏–ª–∏ None
        """
        if not self.ai:
            logger.warning("‚ö†Ô∏è AI –º–µ–Ω–µ–¥–∂–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return None
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            combined_content = "\n\n---\n\n".join([
                f"–ò—Å—Ç–æ—á–Ω–∏–∫: {item['title']}\nURL: {item['url']}\n\n{item['content'][:2000]}"
                for item in content_list
            ])
            
            prompt = f"""–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤:

{combined_content}

–ó–∞–¥–∞—á–∞: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–ò—Å–ø–æ–ª—å–∑—É–π –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏.
–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
            
            # –í—ã–∑—ã–≤–∞–µ–º AI (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è —á—Ç–æ –µ—Å—Ç—å –º–µ—Ç–æ–¥ chat_completion)
            if hasattr(self.ai, 'chat_completion'):
                response = await self.ai.chat_completion(
                    messages=[{"role": "user", "content": prompt}],
                    task_type="analysis"
                )
                return response
            else:
                logger.warning("‚ö†Ô∏è AI –º–µ–Ω–µ–¥–∂–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç chat_completion")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None
    
    def extract_search_query(self, full_query: str) -> str:
        """
        –£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∫–æ–º–∞–Ω–¥—ã.
        
        Args:
            full_query: –ü–æ–ª–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        """
        # –£–¥–∞–ª—è–µ–º –∫–æ–º–∞–Ω–¥—ã –∏ –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
        query = full_query.lower()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ–º–∞–Ω–¥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        command_patterns = [
            r'–æ—Ç–∫—Ä–æ–π –±—Ä–∞—É–∑–µ—Ä\s+(?:–∏\s+)?',
            r'–æ—Ç–∫—Ä–æ–π\s+–±—Ä–∞—É–∑–µ—Ä\s+',
            r'–Ω–∞–π–¥–∏\s+(?:–≤\s+)?(?:–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ\s+)?(?:–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\s+)?(?:–ø—Ä–æ\s+|–æ\s+|–æ–±\s+)?',
            r'–ø–æ–∏—â–∏\s+(?:–≤\s+)?(?:–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ\s+)?(?:–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\s+)?(?:–ø—Ä–æ\s+|–æ\s+|–æ–±\s+)?',
            r'–ø–æ–∏—Å–∫\s+(?:–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\s+)?(?:–ø—Ä–æ\s+|–æ\s+|–æ–±\s+)?',
            r'–∑–∞–≥—É–≥–ª–∏\s+',
            r'–≥—É–≥–ª\s+',
            r'–∏\s+—Ä–∞—Å—Å–∫–∞–∂–∏\s+–º–Ω–µ.*$',
            r'—Ä–∞—Å—Å–∫–∞–∂–∏\s+–º–Ω–µ.*$',
            r'–∏\s+–æ–±—ä—è—Å–Ω–∏.*$',
            r'–æ–±—ä—è—Å–Ω–∏.*$',
        ]
        
        for pattern in command_patterns:
            query = re.sub(pattern, '', query, flags=re.IGNORECASE)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        query = ' '.join(query.split())
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏
        query = query.replace('–ø—Ä–æ—Å', '–ø—Ä–æ')
        query = query.replace('–∏—Ñ–æ—Ä–º–∞—Ç—Ü–∏—é', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é')
        query = query.replace('–∏—Ñ–æ—Ä–º–∞—Ü–∏—é', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é')
        
        return query.strip()
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å —Å–µ—Å—Å–∏—é"""
        self.session.close()
        logger.info("üîí WebScraperAgent –∑–∞–∫—Ä—ã—Ç")


# ============================================================================
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
# ============================================================================

async def search_web(query: str, ai_manager=None) -> Dict[str, Any]:
    """
    –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —Å –∞–Ω–∞–ª–∏–∑–æ–º.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        ai_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä AI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    scraper = WebScraperAgent(ai_manager)
    try:
        result = await scraper.search_and_analyze(query)
        return result
    finally:
        scraper.close()


if __name__ == "__main__":
    # –¢–µ—Å—Ç –∞–≥–µ–Ω—Ç–∞
    async def test():
        scraper = WebScraperAgent()
        result = await scraper.search_and_analyze("Python –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", num_results=2, analyze=False)
        
        print("\n" + "="*70)
        print("üîç –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê")
        print("="*70)
        print(f"–ó–∞–ø—Ä–æ—Å: {result['query']}")
        print(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {result['summary']['total_results']}")
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {result['summary']['scraped_pages']}")
        
        print("\nüìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:")
        for i, res in enumerate(result['search_results'][:5], 1):
            print(f"\n{i}. {res['title']}")
            print(f"   {res['url']}")
            print(f"   {res['snippet'][:100]}...")
        
        if result['scraped_content']:
            print("\nüìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç:")
            for i, content in enumerate(result['scraped_content'], 1):
                print(f"\n{i}. {content['title']}")
                print(f"   –°–∏–º–≤–æ–ª–æ–≤: {len(content['content'])}")
                print(f"   –ü—Ä–µ–≤—å—é: {content['content'][:200]}...")
        
        scraper.close()
    
    asyncio.run(test())
