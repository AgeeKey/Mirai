"""
MIRAI Web Scraper - –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- Playwright –±—Ä–∞—É–∑–µ—Ä (–ø–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ JavaScript)
- BeautifulSoup –ø–∞—Ä—Å–∏–Ω–≥ HTML
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, —Å—Å—ã–ª–æ–∫, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
- –°–∫—Ä–µ–π–ø–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
"""

import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import requests
from typing import Dict, List, Optional
import logging
from urllib.parse import urljoin, urlparse
import re


class MiraiWebScraper:
    """–°–∏—Å—Ç–µ–º–∞ –≤–µ–±-—Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞ –¥–ª—è MIRAI"""

    def __init__(self, headless: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä–µ–π–ø–µ—Ä–∞

        Args:
            headless: –ó–∞–ø—É—Å–∫–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä –≤ headless —Ä–µ–∂–∏–º–µ (–±–µ–∑ GUI)
        """
        self.headless = headless
        self.browser = None
        self.context = None

        logging.info("‚úÖ –í–µ–±-—Å–∫—Ä–µ–π–ø–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def start(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=self.headless)
        self.context = await self.browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        logging.info("üåê Chromium –±—Ä–∞—É–∑–µ—Ä –∑–∞–ø—É—â–µ–Ω")

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å –±—Ä–∞—É–∑–µ—Ä"""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        logging.info("üîå –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")

    async def scrape_url(self, url: str, wait_for: str = None) -> Dict:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            wait_for: CSS —Å–µ–ª–µ–∫—Ç–æ—Ä —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–ª—è –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏

        Returns:
            Dict —Å –¥–∞–Ω–Ω—ã–º–∏: title, text, links, html
        """
        if not self.browser:
            await self.start()

        page = await self.context.new_page()

        try:
            logging.info(f"üì• –ó–∞–≥—Ä—É–∂–∞—é: {url}")
            await page.goto(url, wait_until="networkidle", timeout=30000)

            # –ñ–¥–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if wait_for:
                await page.wait_for_selector(wait_for, timeout=10000)

            # –ü–æ–ª—É—á–∏—Ç—å HTML
            html = await page.content()

            # –ü–∞—Ä—Å–∏—Ç—å —Å BeautifulSoup
            soup = BeautifulSoup(html, "lxml")

            # –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = soup.title.string if soup.title else ""

            # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç (—É–±—Ä–∞—Ç—å —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏)
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text(separator="\n", strip=True)

            # –ò–∑–≤–ª–µ—á—å —Å—Å—ã–ª–∫–∏
            links = []
            for link in soup.find_all("a", href=True):
                absolute_url = urljoin(url, link["href"])
                links.append({"text": link.get_text(strip=True), "url": absolute_url})

            # –ò–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = []
            for img in soup.find_all("img", src=True):
                absolute_url = urljoin(url, img["src"])
                images.append({"alt": img.get("alt", ""), "url": absolute_url})

            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            meta_desc = soup.find("meta", attrs={"name": "description"})
            description = meta_desc.get("content", "") if meta_desc else ""

            result = {
                "url": url,
                "title": title.strip(),
                "description": description,
                "text": text,
                "links": links[:50],  # –ü–µ—Ä–≤—ã–µ 50 —Å—Å—ã–ª–æ–∫
                "images": images[:20],  # –ü–µ—Ä–≤—ã–µ 20 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                "html_length": len(html),
                "text_length": len(text),
            }

            logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {title[:50]}... ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return result

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return {
                "url": url,
                "error": str(e),
                "title": "",
                "text": "",
                "links": [],
                "images": [],
            }
        finally:
            await page.close()

    async def scrape_multiple(self, urls: List[str]) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        if not self.browser:
            await self.start()

        tasks = [self.scrape_url(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results

    async def search_google(self, query: str, num_results: int = 10) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –≤ Google

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            num_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            List —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å title, url, snippet
        """
        if not self.browser:
            await self.start()

        page = await self.context.new_page()

        try:
            search_url = f"https://www.google.com/search?q={query}&num={num_results}"
            logging.info(f"üîç –ü–æ–∏—Å–∫ –≤ Google: {query}")

            await page.goto(search_url, wait_until="networkidle")

            # –ñ–¥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            await page.wait_for_selector("div#search", timeout=10000)

            html = await page.content()
            soup = BeautifulSoup(html, "lxml")

            results = []

            # –ù–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            for g in soup.find_all("div", class_="g"):
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞
                title_elem = g.find("h3")
                link_elem = g.find("a")
                snippet_elem = g.find("div", class_=["VwiC3b", "yXK7lf"])

                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    url = link_elem.get("href", "")
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

                    # –¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ URL
                    if url.startswith("http"):
                        results.append({"title": title, "url": url, "snippet": snippet})

            logging.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return results[:num_results]

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
        finally:
            await page.close()

    async def search_duckduckgo(self, query: str, num_results: int = 10) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –≤ DuckDuckGo (–∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Google)

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            num_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        if not self.browser:
            await self.start()

        page = await self.context.new_page()

        try:
            search_url = f"https://duckduckgo.com/?q={query}"
            logging.info(f"üîç –ü–æ–∏—Å–∫ –≤ DuckDuckGo: {query}")

            await page.goto(search_url, wait_until="networkidle")
            await page.wait_for_selector("article", timeout=10000)

            html = await page.content()
            soup = BeautifulSoup(html, "lxml")

            results = []

            for article in soup.find_all("article", limit=num_results):
                title_elem = article.find("h2")
                link_elem = article.find("a")
                snippet_elem = article.find(
                    ["span", "div"], class_=re.compile("snippet")
                )

                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    url = link_elem.get("href", "")
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

                    results.append({"title": title, "url": url, "snippet": snippet})

            logging.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return results

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
        finally:
            await page.close()

    def scrape_simple(self, url: str) -> Dict:
        """
        –ü—Ä–æ—Å—Ç–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–µ–π–ø–∏–Ω–≥ —á–µ—Ä–µ–∑ requests (–±—ã—Å—Ç—Ä–µ–µ –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Returns:
            Dict —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            logging.info(f"üì• –ó–∞–≥—Ä—É–∂–∞—é (simple): {url}")

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "lxml")

            # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text(separator="\n", strip=True)

            title = soup.title.string if soup.title else ""

            # –°—Å—ã–ª–∫–∏
            links = []
            for link in soup.find_all("a", href=True):
                absolute_url = urljoin(url, link["href"])
                links.append({"text": link.get_text(strip=True), "url": absolute_url})

            result = {
                "url": url,
                "title": title.strip(),
                "text": text,
                "links": links[:50],
                "text_length": len(text),
            }

            logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {title[:50]}... ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return result

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return {"url": url, "error": str(e), "title": "", "text": ""}

    async def extract_article(self, url: str) -> Dict:
        """
        –ò–∑–≤–ª–µ—á—å –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ (—É–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ)

        Args:
            url: URL —Å—Ç–∞—Ç—å–∏

        Returns:
            Dict —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º, –∞–≤—Ç–æ—Ä–æ–º, –¥–∞—Ç–æ–π, —Ç–µ–∫—Å—Ç–æ–º
        """
        result = await self.scrape_url(url)

        if "error" in result:
            return result

        soup = BeautifulSoup(result.get("html", ""), "lxml")

        # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        article = None

        # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ–≥–∏
        for tag in ["article", "main", ["div", {"class": "content"}]]:
            article = soup.find(tag)
            if article:
                break

        if article:
            text = article.get_text(separator="\n", strip=True)
        else:
            text = result["text"]

        # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ –∞–≤—Ç–æ—Ä–∞
        author = ""
        author_elem = soup.find(["meta", "span", "div"], attrs={"name": "author"})
        if author_elem:
            author = author_elem.get("content", "") or author_elem.get_text(strip=True)

        # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ –¥–∞—Ç—É
        date = ""
        date_elem = soup.find("time")
        if date_elem:
            date = date_elem.get("datetime", "") or date_elem.get_text(strip=True)

        return {
            "url": url,
            "title": result.get("title", ""),
            "author": author,
            "date": date,
            "text": text,
            "text_length": len(text),
        }


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    logging.basicConfig(level=logging.INFO)

    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–±-—Å–∫—Ä–µ–π–ø–µ—Ä–∞ MIRAI...\n")

    scraper = MiraiWebScraper()
    await scraper.start()

    # –¢–µ—Å—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Å—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    print("üìù –¢–µ—Å—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ Wikipedia...")
    result = await scraper.scrape_url(
        "https://en.wikipedia.org/wiki/Python_(programming_language)"
    )
    print(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {result['title']}")
    print(f"‚úÖ –¢–µ–∫—Å—Ç–∞: {result['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"‚úÖ –°—Å—ã–ª–æ–∫: {len(result['links'])}")
    print()

    # –¢–µ—Å—Ç 2: –ü–æ–∏—Å–∫ –≤ DuckDuckGo
    print("üìù –¢–µ—Å—Ç 2: –ü–æ–∏—Å–∫ –≤ DuckDuckGo...")
    search_results = await scraper.search_duckduckgo(
        "artificial intelligence", num_results=5
    )
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(search_results)}")
    for i, res in enumerate(search_results, 1):
        print(f"  {i}. {res['title'][:60]}...")
    print()

    # –¢–µ—Å—Ç 3: –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–µ–π–ø–∏–Ω–≥
    print("üìù –¢–µ—Å—Ç 3: –ë—ã—Å—Ç—Ä—ã–π —Å–∫—Ä–µ–π–ø–∏–Ω–≥...")
    simple_result = scraper.scrape_simple("https://example.com")
    print(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {simple_result['title']}")
    print()

    await scraper.close()
    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!")


if __name__ == "__main__":
    asyncio.run(main())
