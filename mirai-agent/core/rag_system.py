"""
MIRAI RAG System - Retrieval Augmented Generation

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –†–∞–∑–±–∏–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏
- –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI Embeddings
- –•—Ä–∞–Ω–µ–Ω–∏–µ –≤ Chroma DB
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
- –†–∞–±–æ—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –±–æ–ª—å—à–µ GPT –ª–∏–º–∏—Ç–∞
"""

import os
from typing import List, Dict, Optional
from pathlib import Path
import logging

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
import tiktoken


class MiraiRAG:
    """
    RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è MIRAI

    –ü–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –ª—é–±–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —á–µ—Ä–µ–∑:
    1. –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ (chunks)
    2. –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç–µ–π
    4. –ü–æ–¥–∞—á–∞ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ GPT
    """

    def __init__(
        self,
        collection_name: str = "mirai_knowledge",
        persist_directory: str = "data/chroma_db",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã

        Args:
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Chroma
            persist_directory: –ü–∞–ø–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ë–î
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        """
        self.collection_name = collection_name
        self.persist_directory = Path(persist_directory)
        self.persist_directory.mkdir(parents=True, exist_ok=True)

        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å OpenAI API –∫–ª—é—á
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è!")

        # –°–æ–∑–¥–∞—Ç—å embeddings
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"  # –î–µ—à–µ–≤–ª–µ —á–µ–º ada-002
        )

        # Text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""],
        )

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å vectorstore
        try:
            self.vectorstore = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory=str(self.persist_directory),
            )
            logging.info(f"‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {collection_name}")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è –°–æ–∑–¥–∞—é –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é: {e}")
            self.vectorstore = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory=str(self.persist_directory),
            )

        # Tokenizer –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-4")
        except:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        return len(self.tokenizer.encode(text))

    def add_text(
        self, text: str, metadata: Dict = None, source: str = "unknown"
    ) -> int:
        """
        –î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –≤ RAG —Å–∏—Å—Ç–µ–º—É

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            source: –ò—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–∫—Å—Ç–∞

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        logging.info(f"üìù –î–æ–±–∞–≤–ª—è—é —Ç–µ–∫—Å—Ç –≤ RAG ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)...")

        # –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.text_splitter.split_text(text)

        # –°–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = []
        for i, chunk in enumerate(chunks):
            doc_metadata = {
                "source": source,
                "chunk_index": i,
                "total_chunks": len(chunks),
                "chunk_length": len(chunk),
                "tokens": self.count_tokens(chunk),
            }

            # –î–æ–±–∞–≤–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if metadata:
                doc_metadata.update(metadata)

            documents.append(Document(page_content=chunk, metadata=doc_metadata))

        # –î–æ–±–∞–≤–∏—Ç—å –≤ vectorstore
        self.vectorstore.add_documents(documents)

        total_tokens = sum(doc.metadata["tokens"] for doc in documents)
        logging.info(
            f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ "
            f"({total_tokens} —Ç–æ–∫–µ–Ω–æ–≤) –≤ RAG —Å–∏—Å—Ç–µ–º—É"
        )

        return len(chunks)

    def add_file(self, file_path: str, metadata: Dict = None) -> int:
        """
        –î–æ–±–∞–≤–∏—Ç—å —Ñ–∞–π–ª –≤ RAG —Å–∏—Å—Ç–µ–º—É

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

        logging.info(f"üìÑ –ó–∞–≥—Ä—É–∂–∞—é —Ñ–∞–π–ª: {file_path.name}")

        # –ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()

        # –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
        file_metadata = {
            "filename": file_path.name,
            "filepath": str(file_path),
            "file_size": len(text),
        }

        if metadata:
            file_metadata.update(metadata)

        return self.add_text(text, metadata=file_metadata, source=str(file_path))

    def search(
        self, query: str, k: int = 5, filter_metadata: Dict = None
    ) -> List[Document]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –≤ RAG —Å–∏—Å—Ç–µ–º–µ

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            filter_metadata: –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º

        Returns:
            List —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        logging.info(f"üîç –ü–æ–∏—Å–∫ –≤ RAG: '{query[:50]}...' (k={k})")

        results = self.vectorstore.similarity_search(query, k=k, filter=filter_metadata)

        total_tokens = sum(self.count_tokens(doc.page_content) for doc in results)
        logging.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —á–∞–Ω–∫–æ–≤ ({total_tokens} —Ç–æ–∫–µ–Ω–æ–≤)")

        return results

    def search_with_scores(self, query: str, k: int = 5) -> List[tuple]:
        """
        –ü–æ–∏—Å–∫ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏

        Returns:
            List[(Document, score)] –≥–¥–µ score - –ø–æ—Ö–æ–∂–µ—Å—Ç—å (0-1)
        """
        results = self.vectorstore.similarity_search_with_score(query, k=k)

        logging.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        for i, (doc, score) in enumerate(results, 1):
            source = doc.metadata.get("source", "unknown")
            logging.info(f"  {i}. Score: {score:.3f} | Source: {source[:50]}")

        return results

    def get_context_for_query(
        self, query: str, max_tokens: int = 4000, k: int = 10
    ) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞

        –°–æ–±–∏—Ä–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –ø–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞

        Returns:
            –°–æ–±—Ä–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        """
        # –ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
        results = self.search(query, k=k)

        context_parts = []
        total_tokens = 0

        for doc in results:
            chunk = doc.page_content
            tokens = self.count_tokens(chunk)

            if total_tokens + tokens <= max_tokens:
                context_parts.append(chunk)
                total_tokens += tokens
            else:
                break

        context = "\n\n---\n\n".join(context_parts)

        logging.info(
            f"‚úÖ –°–æ–±—Ä–∞–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç: {len(context_parts)} —á–∞–Ω–∫–æ–≤, "
            f"{total_tokens} —Ç–æ–∫–µ–Ω–æ–≤"
        )

        return context

    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É RAG —Å–∏—Å—Ç–µ–º—ã"""
        collection = self.vectorstore._collection
        count = collection.count()

        return {
            "collection_name": self.collection_name,
            "total_chunks": count,
            "persist_directory": str(self.persist_directory),
        }

    def clear(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –≤—Å—é –∫–æ–ª–ª–µ–∫—Ü–∏—é"""
        self.vectorstore.delete_collection()
        logging.warning("üóëÔ∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è –æ—á–∏—â–µ–Ω–∞!")

    def delete_by_source(self, source: str):
        """–£–¥–∞–ª–∏—Ç—å –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        # Chroma –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –Ω–∞–ø—Ä—è–º—É—é
        # –ù—É–∂–Ω–æ –Ω–∞–π—Ç–∏ ID –∏ —É–¥–∞–ª–∏—Ç—å
        logging.warning(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ source: {source}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    logging.basicConfig(level=logging.INFO)

    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã MIRAI...\n")

    # –°–æ–∑–¥–∞—Ç—å RAG
    rag = MiraiRAG(collection_name="test_knowledge")

    # –¢–µ—Å—Ç 1: –î–æ–±–∞–≤–∏—Ç—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    print("üìù –¢–µ—Å—Ç 1: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞...")
    long_text = (
        """
    Python - —ç—Ç–æ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è.
    –û–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω –ì–≤–∏–¥–æ –≤–∞–Ω –†–æ—Å—Å—É–º–æ–º –≤ 1991 –≥–æ–¥—É.
    Python –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–µ–π –ø—Ä–æ—Å—Ç–æ—Ç–æ–π –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å—é –∫–æ–¥–∞.
    
    –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Python:
    1. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è
    2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é
    3. –ë–æ–≥–∞—Ç–∞—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
    4. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–¥–∏–≥–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
    
    Python –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö:
    - –í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ (Django, Flask)
    - –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (TensorFlow, PyTorch)
    - –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (Pandas, NumPy)
    - –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —Å–∫—Ä–∏–ø—Ç–∏–Ω–≥
    
    """
        * 3
    )  # –ü–æ–≤—Ç–æ—Ä–∏—Ç—å 3 —Ä–∞–∑–∞ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ–±—ä–µ–º–∞

    chunks_added = rag.add_text(
        long_text,
        metadata={"category": "programming", "language": "ru"},
        source="python_intro.txt",
    )
    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {chunks_added}\n")

    # –¢–µ—Å—Ç 2: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    print("üìù –¢–µ—Å—Ç 2: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫...")
    results = rag.search("–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", k=3)
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
    for i, doc in enumerate(results, 1):
        preview = doc.page_content[:100].replace("\n", " ")
        print(f"  {i}. {preview}...")
    print()

    # –¢–µ—Å—Ç 3: –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    print("üìù –¢–µ—Å—Ç 3: –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
    context = rag.get_context_for_query(
        "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Python", max_tokens=500
    )
    print(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–±—Ä–∞–Ω: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–ü—Ä–µ–≤—å—é: {context[:200]}...\n")

    # –¢–µ—Å—Ç 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("üìù –¢–µ—Å—Ç 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ RAG...")
    stats = rag.get_stats()
    print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    for key, value in stats.items():
        print(f"  - {key}: {value}")
    print()

    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã RAG –ø—Ä–æ–π–¥–µ–Ω—ã!")


if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å .env –¥–ª—è OpenAI –∫–ª—é—á–∞
    from dotenv import load_dotenv

    load_dotenv()

    main()
