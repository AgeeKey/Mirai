"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç –≤—Å–µ—Ö —É–ª—É—á—à–µ–Ω–∏–π MIRAI

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç:
1. –ü–∞–º—è—Ç—å (SQLite)
2. –í–µ–±-—Å–∫—Ä–µ–π–ø–∏–Ω–≥ (Playwright)
3. RAG —Å–∏—Å—Ç–µ–º–∞ (Chroma)
"""

import asyncio
import logging
from pathlib import Path

from core.memory import MiraiMemory
from core.web_scraper import MiraiWebScraper
from core.rag_system import MiraiRAG


async def comprehensive_test():
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º"""

    print("=" * 60)
    print("üß™ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –¢–ï–°–¢ –í–°–ï–• –£–õ–£–ß–®–ï–ù–ò–ô MIRAI")
    print("=" * 60)
    print()

    # === –¢–ï–°–¢ 1: –ü–ê–ú–Ø–¢–¨ ===
    print("üìù –¢–ï–°–¢ 1: –°–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ (SQLite)")
    print("-" * 60)

    memory = MiraiMemory()

    # –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—É—é –∑–∞–¥–∞—á—É
    task_id = memory.create_task(
        "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç", "–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö —É–ª—É—á—à–µ–Ω–∏–π MIRAI", priority="high"
    )
    memory.start_task(task_id)

    # –ó–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è
    memory.log_action("test_memory", {"status": "working"}, task_id=task_id)

    # –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = memory.get_statistics()
    print(f"‚úÖ –ü–∞–º—è—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç!")
    print(f"  - –ó–∞–¥–∞—á –≤ –ë–î: {sum(stats['tasks_by_status'].values())}")
    print(f"  - –î–µ–π—Å—Ç–≤–∏–π: {stats['total_actions']}")
    print(f"  - –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {stats['success_rate']:.1f}%")
    print()

    # === –¢–ï–°–¢ 2: –í–ï–ë-–°–ö–†–ï–ô–ü–ò–ù–ì ===
    print("üìù –¢–ï–°–¢ 2: –í–µ–±-—Å–∫—Ä–µ–π–ø–∏–Ω–≥ (Playwright + BeautifulSoup)")
    print("-" * 60)

    scraper = MiraiWebScraper(headless=True)
    await scraper.start()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    url = "https://en.wikipedia.org/wiki/Artificial_intelligence"
    result = await scraper.scrape_url(url)

    print(f"‚úÖ –í–µ–±-—Å–∫—Ä–µ–π–ø–∏–Ω–≥ —Ä–∞–±–æ—Ç–∞–µ—Ç!")
    print(f"  - URL: {url}")
    print(f"  - –ó–∞–≥–æ–ª–æ–≤–æ–∫: {result['title'][:50]}...")
    print(f"  - –¢–µ–∫—Å—Ç–∞: {result['text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  - –°—Å—ã–ª–æ–∫: {len(result['links'])}")

    # –ó–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –≤ –ø–∞–º—è—Ç—å
    memory.log_action(
        "web_scraping",
        {"url": url, "text_length": result["text_length"]},
        task_id=task_id,
    )

    print()

    # === –¢–ï–°–¢ 3: RAG –°–ò–°–¢–ï–ú–ê ===
    print("üìù –¢–ï–°–¢ 3: RAG —Å–∏—Å—Ç–µ–º–∞ (Chroma + Embeddings)")
    print("-" * 60)

    rag = MiraiRAG(collection_name="test_integration")

    # –î–æ–±–∞–≤–∏—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ RAG
    chunks_added = rag.add_text(
        result["text"][:5000],  # –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤
        metadata={"source_url": url, "title": result["title"]},
        source="wikipedia_ai",
    )

    print(f"‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç!")
    print(f"  - –¢–µ–∫—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω: 5,000 —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  - –ß–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {chunks_added}")

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    search_results = rag.search("What is machine learning?", k=3)

    print(f"  - –ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç: {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

    # –ó–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –≤ –ø–∞–º—è—Ç—å
    memory.log_action(
        "rag_indexing",
        {"chunks": chunks_added, "search_results": len(search_results)},
        task_id=task_id,
    )

    print()

    # === –§–ò–ù–ê–õ–¨–ù–ê–Ø –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø ===
    print("üìù –¢–ï–°–¢ 4: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º")
    print("-" * 60)

    # –°—Ü–µ–Ω–∞—Ä–∏–π: –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Ç–µ–º—É –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–Ω–∞–Ω–∏—è

    # 1. –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
    print("1Ô∏è‚É£ –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ...")
    search_query = "artificial intelligence"
    search_results_web = await scraper.search_duckduckgo(search_query, num_results=3)
    print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(search_results_web)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

    # 2. –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç–∞—Ç—å—é
    if search_results_web:
        first_url = search_results_web[0].get("url", url)
        print(f"2Ô∏è‚É£ –ó–∞–≥—Ä—É–∂–∞—é –ø–µ—Ä–≤—É—é —Å—Ç–∞—Ç—å—é...")
        try:
            article = await scraper.scrape_url(first_url)
            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {article['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
        except:
            article = result  # Fallback –∫ Wikipedia
            print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback: Wikipedia")
    else:
        article = result
        print(f"2Ô∏è‚É£ –ò—Å–ø–æ–ª—å–∑—É–µ–º Wikipedia –∫–∞–∫ fallback")

    # 3. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ RAG
    print("3Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω—è—é –≤ RAG —Å–∏—Å—Ç–µ–º—É...")
    rag_chunks = rag.add_text(
        article["text"][:3000], metadata={"topic": "AI"}, source="integration_test"
    )
    print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {rag_chunks} —á–∞–Ω–∫–æ–≤")

    # 4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–Ω–∞–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å
    print("4Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω—è—é –∑–Ω–∞–Ω–∏–µ –≤ –ë–î...")
    learning_id = memory.save_learning(
        topic="Artificial Intelligence",
        content=f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–æ {len(search_results_web)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
        source="web_research",
        confidence=0.8,
    )
    print(f"   ID –∑–Ω–∞–Ω–∏—è: {learning_id}")

    # 5. –ó–∞–≤–µ—Ä—à–∏—Ç—å –∑–∞–¥–∞—á—É
    print("5Ô∏è‚É£ –ó–∞–≤–µ—Ä—à–∞—é –∑–∞–¥–∞—á—É...")
    memory.complete_task(
        task_id,
        result=f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {article['text_length']} —Å–∏–º–≤–æ–ª–æ–≤, —Å–æ–∑–¥–∞–Ω–æ {rag_chunks} —á–∞–Ω–∫–æ–≤",
    )
    print("   –ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

    print()

    # === –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===
    print("=" * 60)
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 60)

    final_stats = memory.get_statistics()
    rag_stats = rag.get_stats()

    print(f"\nüíæ –ü–ê–ú–Ø–¢–¨:")
    print(f"  - –ó–∞–¥–∞—á: {sum(final_stats['tasks_by_status'].values())}")
    print(f"  - –î–µ–π—Å—Ç–≤–∏–π: {final_stats['total_actions']}")
    print(f"  - –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {final_stats['total_results']}")
    print(f"  - –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {final_stats['success_rate']:.1f}%")

    print(f"\nüåê –í–ï–ë-–°–ö–†–ï–ô–ü–ò–ù–ì:")
    print(f"  - –°—Ç—Ä–∞–Ω–∏—Ü –∑–∞–≥—Ä—É–∂–µ–Ω–æ: 2+")
    print(f"  - –°–∏–º–≤–æ–ª–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {article['text_length']:,}")
    print(f"  - –°—Å—ã–ª–æ–∫ –Ω–∞–π–¥–µ–Ω–æ: {len(article['links'])}")

    print(f"\nüß† RAG –°–ò–°–¢–ï–ú–ê:")
    print(f"  - –ö–æ–ª–ª–µ–∫—Ü–∏—è: {rag_stats['collection_name']}")
    print(f"  - –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {rag_stats['total_chunks']}")
    print(f"  - –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {rag_stats['persist_directory']}")

    print()
    print("=" * 60)
    print("‚úÖ –í–°–ï –°–ò–°–¢–ï–ú–´ –†–ê–ë–û–¢–ê–Æ–¢ –ò–î–ï–ê–õ–¨–ù–û!")
    print("=" * 60)
    print()

    # –ó–∞–∫—Ä—ã—Ç—å –±—Ä–∞—É–∑–µ—Ä
    await scraper.close()

    return {
        "memory": final_stats,
        "rag": rag_stats,
        "scraping": {"pages_loaded": 2, "text_length": article["text_length"]},
    }


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å .env
    from dotenv import load_dotenv

    load_dotenv()

    # –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç
    results = asyncio.run(comprehensive_test())

    print("\nüìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ UPGRADE_COMPLETE.md")
