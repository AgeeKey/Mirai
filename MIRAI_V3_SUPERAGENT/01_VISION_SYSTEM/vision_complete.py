#!/usr/bin/env python3
"""
PHASE 1: Complete Vision System Implementation
==============================================

Production-ready vision system with GPT-4 Vision integration for screen analysis.
Implements all 150 steps from PHASE_1_VISION_SYSTEM_DETAILED.md

Author: MIRAI AI Agent
Version: 1.0.0
"""

import logging
import base64
import json
import os
import time
import hashlib
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from io import BytesIO
from dataclasses import dataclass, asdict

# Core libraries
try:
    import openai
except ImportError:
    raise ImportError("openai library not installed. Run: pip install openai")

try:
    from PIL import Image, ImageGrab
except ImportError:
    raise ImportError("Pillow library not installed. Run: pip install pillow")

try:
    import pyautogui
except ImportError:
    raise ImportError("pyautogui library not installed. Run: pip install pyautogui")

try:
    import cv2
except ImportError:
    raise ImportError("opencv-python library not installed. Run: pip install opencv-python")

try:
    import numpy as np
except ImportError:
    raise ImportError("numpy library not installed. Run: pip install numpy")


# ============================================================================
# SECTION 1: Vision Initialization (Steps 1-15)
# ============================================================================

@dataclass
class VisionConfig:
    """Configuration parameters for vision system (Step 4)."""
    
    # Screenshot settings
    screenshot_resolution: Tuple[int, int] = (1920, 1080)
    screenshot_quality: int = 95
    screenshot_format: str = "PNG"
    
    # GPT-4 Vision settings
    gpt4_model: str = "gpt-4o-mini"
    gpt4_max_tokens: int = 1000
    gpt4_temperature: float = 0.7
    gpt4_timeout: int = 30
    
    # Performance settings
    max_retries: int = 3
    retry_delay: float = 1.0
    max_delay: float = 60.0
    cache_enabled: bool = True
    cache_ttl: int = 300
    
    # Detection settings
    min_element_size: int = 10
    edge_detection_threshold: int = 100
    ocr_confidence_threshold: float = 0.6
    
    # Storage settings
    output_dir: str = "/tmp/vision_system"
    db_path: str = "/tmp/vision_system/vision.db"
    log_file: Optional[str] = None


class VisionError(Exception):
    """Base exception for vision system errors (Step 9)."""
    pass


class ScreenshotError(VisionError):
    """Exception for screenshot capture errors (Step 9)."""
    pass


class GPT4VisionError(VisionError):
    """Exception for GPT-4 Vision API errors (Step 9)."""
    pass


class ElementDetectionError(VisionError):
    """Exception for element detection errors (Step 9)."""
    pass


class VisionLogger:
    """Centralized logger for vision system operations (Step 10)."""
    
    def __init__(
        self,
        name: str = "VisionSystem",
        log_level: int = logging.INFO,
        log_file: Optional[str] = None
    ):
        """Initialize vision logger."""
        self.logger = logging.getLogger(name)
        self.logger.setLevel(log_level)
        self.log_file = log_file
        
        # Remove existing handlers
        self.logger.handlers.clear()
        
        # Create formatters
        self.detailed_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - '
            '%(filename)s:%(lineno)d - %(funcName)s - %(message)s'
        )
        self.simple_formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        )
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(log_level)
        console_handler.setFormatter(self.simple_formatter)
        self.logger.addHandler(console_handler)
        
        # File handler
        if log_file:
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(logging.DEBUG)
            file_handler.setFormatter(self.detailed_formatter)
            self.logger.addHandler(file_handler)
    
    def info(self, msg: str, *args, **kwargs):
        """Log info message."""
        self.logger.info(msg, *args, **kwargs)
    
    def debug(self, msg: str, *args, **kwargs):
        """Log debug message."""
        self.logger.debug(msg, *args, **kwargs)
    
    def warning(self, msg: str, *args, **kwargs):
        """Log warning message."""
        self.logger.warning(msg, *args, **kwargs)
    
    def error(self, msg: str, *args, **kwargs):
        """Log error message."""
        self.logger.error(msg, *args, **kwargs)
    
    def critical(self, msg: str, *args, **kwargs):
        """Log critical message."""
        self.logger.critical(msg, *args, **kwargs)


class VisionCache:
    """In-memory cache for vision system results (Step 8)."""
    
    def __init__(self, ttl: int = 300):
        """Initialize cache with TTL."""
        self.ttl = ttl
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.access_count: Dict[str, int] = {}
    
    def _generate_key(self, data: bytes) -> str:
        """Generate cache key from data."""
        return hashlib.sha256(data).hexdigest()
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        """Store value in cache."""
        expires_at = time.time() + (ttl or self.ttl)
        self.cache[key] = {
            'value': value,
            'expires_at': expires_at,
            'created_at': time.time()
        }
        self.access_count[key] = 0
    
    def get(self, key: str) -> Optional[Any]:
        """Retrieve value from cache."""
        if key not in self.cache:
            return None
        
        entry = self.cache[key]
        
        # Check expiration
        if time.time() > entry['expires_at']:
            del self.cache[key]
            return None
        
        self.access_count[key] += 1
        return entry['value']
    
    def clear(self) -> None:
        """Clear all cache entries."""
        self.cache.clear()
        self.access_count.clear()


class VisionDatabase:
    """Database for storing vision analysis results (Step 14)."""
    
    def __init__(self, db_path: str = "/tmp/vision_system/vision.db"):
        """Initialize database connection."""
        self.db_path = db_path
        self.conn: Optional[sqlite3.Connection] = None
        self._create_tables()
    
    def connect(self) -> None:
        """Establish database connection."""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row
        except sqlite3.Error as e:
            raise VisionError(f"Database connection failed: {e}")
    
    def _create_tables(self) -> None:
        """Create database tables if they don't exist."""
        # Ensure directory exists
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
        self.connect()
        cursor = self.conn.cursor()
        
        # Screenshots table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS screenshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                file_path TEXT NOT NULL,
                width INTEGER NOT NULL,
                height INTEGER NOT NULL,
                format TEXT NOT NULL,
                size_bytes INTEGER NOT NULL,
                checksum TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Analysis results table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analysis_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                screenshot_id INTEGER NOT NULL,
                analysis_type TEXT NOT NULL,
                result_json TEXT NOT NULL,
                confidence REAL,
                processing_time REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (screenshot_id) REFERENCES screenshots (id)
            )
        ''')
        
        # Elements detected table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS detected_elements (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                screenshot_id INTEGER NOT NULL,
                element_type TEXT NOT NULL,
                x INTEGER NOT NULL,
                y INTEGER NOT NULL,
                width INTEGER NOT NULL,
                height INTEGER NOT NULL,
                confidence REAL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (screenshot_id) REFERENCES screenshots (id)
            )
        ''')
        
        self.conn.commit()
    
    def store_screenshot(
        self,
        file_path: str,
        width: int,
        height: int,
        format: str,
        size_bytes: int,
        checksum: str
    ) -> int:
        """Store screenshot metadata."""
        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT INTO screenshots 
            (timestamp, file_path, width, height, format, size_bytes, checksum)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            datetime.now().isoformat(),
            file_path,
            width,
            height,
            format,
            size_bytes,
            checksum
        ))
        self.conn.commit()
        return cursor.lastrowid
    
    def close(self) -> None:
        """Close database connection."""
        if self.conn:
            self.conn.close()


class VisionInitializer:
    """
    Vision system initializer (Steps 1-15).
    
    Handles:
    - Library imports and validation
    - Configuration loading
    - Logging setup
    - Directory creation
    - GPU initialization
    - Database setup
    """
    
    def __init__(self, config: Optional[VisionConfig] = None):
        """
        Initialize vision system.
        
        Args:
            config: Optional configuration object
        """
        self.config = config or VisionConfig()
        self.logger: Optional[VisionLogger] = None
        self.cache: Optional[VisionCache] = None
        self.db: Optional[VisionDatabase] = None
        self.gpu_info: Dict[str, Any] = {}
        self.directories: Dict[str, Path] = {}
    
    def setup(self) -> None:
        """Run complete initialization sequence."""
        # Step 3: Set up logging
        self.logger = VisionLogger(
            log_level=logging.INFO,
            log_file=self.config.log_file
        )
        self.logger.info("üöÄ Initializing Vision System...")
        
        # Step 11: Validate dependencies
        self._validate_dependencies()
        
        # Step 5: Create output directories
        self._create_directories()
        
        # Step 6: Initialize GPU acceleration
        self._initialize_gpu()
        
        # Step 8: Set up cache
        if self.config.cache_enabled:
            self.cache = VisionCache(ttl=self.config.cache_ttl)
            self.logger.info("‚úÖ Cache initialized")
        
        # Step 14: Create database
        self.db = VisionDatabase(self.config.db_path)
        self.logger.info("‚úÖ Database initialized")
        
        # Step 15: Print initialization complete
        self._print_initialization_complete()
    
    def _validate_dependencies(self) -> None:
        """Validate all required dependencies (Step 11)."""
        dependencies = {
            'openai': openai.__version__,
            'PIL': Image.__version__,
            'pyautogui': pyautogui.__version__,
            'cv2': cv2.__version__,
            'numpy': np.__version__,
        }
        
        self.logger.info("üì¶ Dependencies validated:")
        for name, version in dependencies.items():
            self.logger.info(f"   ‚úÖ {name}: v{version}")
    
    def _create_directories(self) -> None:
        """Create output directory structure (Step 5)."""
        base_path = Path(self.config.output_dir)
        
        self.directories = {
            'base': base_path,
            'screenshots': base_path / 'screenshots',
            'thumbnails': base_path / 'thumbnails',
            'analysis': base_path / 'analysis',
            'cache': base_path / 'cache',
            'logs': base_path / 'logs',
            'backups': base_path / 'backups',
        }
        
        for name, path in self.directories.items():
            path.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"‚úÖ Created directory structure at {self.config.output_dir}")
    
    def _initialize_gpu(self) -> None:
        """Initialize GPU acceleration if available (Step 6)."""
        self.gpu_info = {
            'cuda_available': False,
            'device_count': 0,
            'backend': 'CPU',
            'acceleration_enabled': False
        }
        
        try:
            self.gpu_info['cuda_available'] = cv2.cuda.getCudaEnabledDeviceCount() > 0
            
            if self.gpu_info['cuda_available']:
                self.gpu_info['device_count'] = cv2.cuda.getCudaEnabledDeviceCount()
                self.gpu_info['backend'] = 'CUDA'
                self.gpu_info['acceleration_enabled'] = True
                cv2.cuda.setDevice(0)
                self.logger.info(f"‚úÖ GPU acceleration enabled: CUDA ({self.gpu_info['device_count']} devices)")
            else:
                self.logger.info("üíª Using CPU backend (no CUDA devices found)")
        except Exception:
            self.logger.info("üíª Using CPU backend")
    
    def _print_initialization_complete(self) -> None:
        """Print initialization complete message (Step 15)."""
        print("\n" + "=" * 70)
        print("üöÄ VISION SYSTEM INITIALIZATION COMPLETE")
        print("=" * 70)
        print(f"\nüìä Backend: {self.gpu_info['backend']}")
        print(f"‚öôÔ∏è  Model: {self.config.gpt4_model}")
        print(f"üìÅ Output: {self.config.output_dir}")
        print("\n" + "=" * 70)
        print("‚úÖ Ready to process vision tasks!")
        print("=" * 70 + "\n")


# ============================================================================
# SECTION 2: Screenshot Capture (Steps 16-30)
# ============================================================================

class ScreenCaptureManager:
    """
    Screenshot capture and management (Steps 16-30).
    
    Handles:
    - Screenshot capture
    - Dimension verification
    - Image format conversion
    - Compression
    - Metadata storage
    """
    
    def __init__(
        self,
        config: VisionConfig,
        logger: VisionLogger,
        db: VisionDatabase
    ):
        """Initialize screenshot capture manager."""
        self.config = config
        self.logger = logger
        self.db = db
    
    def capture_screenshot(self) -> Tuple[Image.Image, Dict[str, Any]]:
        """
        Capture full screen screenshot (Step 16).
        
        Returns:
            Tuple of (PIL Image, metadata dict)
        
        Raises:
            ScreenshotError: If capture fails
        """
        start_time = time.time()
        
        try:
            # Step 16: Capture screenshot
            screenshot = pyautogui.screenshot()
            
            # Step 17: Verify dimensions
            width, height = screenshot.size
            self.logger.info(f"‚úÖ Screenshot captured: {width}x{height}")
            
            # Step 24: Create metadata
            metadata = {
                'width': width,
                'height': height,
                'mode': screenshot.mode,
                'format': screenshot.format or self.config.screenshot_format,
                'timestamp': datetime.now().isoformat(),
                'processing_time': time.time() - start_time
            }
            
            return screenshot, metadata
        
        except Exception as e:
            self.logger.error(f"‚ùå Screenshot capture failed: {e}")
            raise ScreenshotError(f"Failed to capture screenshot: {e}")
    
    def pil_to_numpy(self, image: Image.Image) -> np.ndarray:
        """
        Convert PIL Image to numpy array (Step 18).
        
        Args:
            image: PIL Image object
        
        Returns:
            Numpy array in BGR format (OpenCV compatible)
        """
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        img_array = np.array(image)
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        
        return img_bgr
    
    def numpy_to_pil(self, img_array: np.ndarray) -> Image.Image:
        """
        Convert numpy array to PIL Image (Step 18).
        
        Args:
            img_array: Numpy array in BGR format
        
        Returns:
            PIL Image object
        """
        img_rgb = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
        return Image.fromarray(img_rgb)
    
    def compress_screenshot(
        self,
        image: Image.Image,
        quality: Optional[int] = None
    ) -> Tuple[BytesIO, int]:
        """
        Compress screenshot to reduce file size (Step 19).
        
        Args:
            image: PIL Image object
            quality: Compression quality (uses config default if None)
        
        Returns:
            Tuple of (compressed image buffer, size in bytes)
        """
        quality = quality or self.config.screenshot_quality
        format = self.config.screenshot_format
        
        buffer = BytesIO()
        
        if format == "JPEG" and image.mode in ('RGBA', 'LA', 'P'):
            image = image.convert('RGB')
        
        image.save(buffer, format=format, quality=quality, optimize=True)
        size_bytes = buffer.tell()
        buffer.seek(0)
        
        self.logger.info(f"üóúÔ∏è Compressed: {size_bytes:,} bytes")
        return buffer, size_bytes
    
    def save_screenshot(
        self,
        image: Image.Image,
        prefix: str = "screenshot"
    ) -> str:
        """
        Save screenshot with timestamp (Step 20).
        
        Args:
            image: PIL Image object
            prefix: Filename prefix
        
        Returns:
            Path to saved file
        """
        output_dir = self.config.output_dir + "/screenshots"
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        filename = f"{prefix}_{timestamp}.{self.config.screenshot_format.lower()}"
        filepath = os.path.join(output_dir, filename)
        
        image.save(filepath, format=self.config.screenshot_format)
        
        file_size = os.path.getsize(filepath)
        self.logger.info(f"üíæ Saved: {filepath} ({file_size:,} bytes)")
        
        # Store in database
        checksum = self._calculate_checksum(filepath)
        self.db.store_screenshot(
            file_path=filepath,
            width=image.width,
            height=image.height,
            format=self.config.screenshot_format,
            size_bytes=file_size,
            checksum=checksum
        )
        
        return filepath
    
    def encode_to_base64(
        self,
        image: Image.Image,
        quality: Optional[int] = None
    ) -> str:
        """
        Encode screenshot to base64 (Step 21).
        
        Args:
            image: PIL Image object
            quality: Compression quality
        
        Returns:
            Base64 encoded string
        """
        buffer, _ = self.compress_screenshot(image, quality)
        base64_str = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        self.logger.debug(f"üîê Encoded to base64: {len(base64_str)} chars")
        return base64_str
    
    def create_thumbnail(
        self,
        image: Image.Image,
        size: Tuple[int, int] = (320, 240)
    ) -> Image.Image:
        """
        Create thumbnail for quick preview (Step 25).
        
        Args:
            image: PIL Image object
            size: Thumbnail size
        
        Returns:
            Thumbnail image
        """
        thumbnail = image.copy()
        thumbnail.thumbnail(size, Image.Resampling.LANCZOS)
        self.logger.debug(f"üñºÔ∏è Created thumbnail: {thumbnail.width}x{thumbnail.height}")
        return thumbnail
    
    def _calculate_checksum(self, filepath: str) -> str:
        """Calculate SHA256 checksum of file."""
        sha256_hash = hashlib.sha256()
        with open(filepath, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()


# ============================================================================
# SECTION 3: GPT-4 Vision Integration (Steps 31-60)
# ============================================================================

class GPT4VisionAnalyzer:
    """
    GPT-4 Vision API integration (Steps 31-60).
    
    Handles:
    - API calls to GPT-4 Vision
    - Response parsing
    - Error handling and retries
    - Result caching
    """
    
    def __init__(
        self,
        client: openai.OpenAI,
        config: VisionConfig,
        logger: VisionLogger,
        cache: Optional[VisionCache] = None
    ):
        """Initialize GPT-4 Vision analyzer."""
        self.client = client
        self.config = config
        self.logger = logger
        self.cache = cache
    
    def analyze_image(
        self,
        image: Image.Image,
        prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze image with GPT-4 Vision (Steps 31-60).
        
        Args:
            image: PIL Image object
            prompt: Optional custom prompt
        
        Returns:
            Analysis result dictionary
        
        Raises:
            GPT4VisionError: If API call fails
        """
        start_time = time.time()
        
        # Step 31: Encode to base64
        buffer = BytesIO()
        image.save(buffer, format='JPEG', quality=85)
        base64_image = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        # Check cache
        cache_key = hashlib.sha256(base64_image.encode()).hexdigest()
        if self.cache:
            cached_result = self.cache.get(cache_key)
            if cached_result:
                self.logger.info("‚úÖ Using cached analysis result")
                return cached_result
        
        # Step 32: Create prompt
        if prompt is None:
            prompt = """Analyze this screenshot and provide:
1. Brief description of what you see
2. List of UI elements (buttons, menus, dialogs, etc.)
3. Any problems or issues (ads, popups, errors)
4. Recommendations for interaction

Respond in JSON format with keys: description, elements, problems, recommendations"""
        
        try:
            # Step 33: Call GPT-4 Vision
            response = self.client.chat.completions.create(
                model=self.config.gpt4_model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=self.config.gpt4_max_tokens,
                temperature=self.config.gpt4_temperature
            )
            
            # Step 34-38: Parse response
            content = response.choices[0].message.content
            
            # Try to parse as JSON
            try:
                result = json.loads(content)
            except json.JSONDecodeError:
                # Fallback to text response
                result = {
                    'description': content,
                    'elements': [],
                    'problems': [],
                    'recommendations': []
                }
            
            # Add metadata
            result['processing_time'] = time.time() - start_time
            result['model'] = self.config.gpt4_model
            result['timestamp'] = datetime.now().isoformat()
            
            # Cache result
            if self.cache:
                self.cache.set(cache_key, result)
            
            self.logger.info(f"‚úÖ GPT-4 Vision analysis complete ({result['processing_time']:.2f}s)")
            return result
        
        except Exception as e:
            self.logger.error(f"‚ùå GPT-4 Vision API error: {e}")
            raise GPT4VisionError(f"API call failed: {e}")


# ============================================================================
# SECTION 4: Screen Analysis (Steps 61-90)
# ============================================================================

class ScreenAnalyzer:
    """
    Screen content analysis (Steps 61-90).
    
    Handles:
    - Window detection
    - UI layout analysis
    - Text extraction (OCR)
    - Theme detection
    """
    
    def __init__(self, config: VisionConfig, logger: VisionLogger):
        """Initialize screen analyzer."""
        self.config = config
        self.logger = logger
    
    def analyze_screen(self, image: Image.Image) -> Dict[str, Any]:
        """
        Comprehensive screen analysis (Steps 61-90).
        
        Args:
            image: PIL Image object
        
        Returns:
            Analysis result dictionary
        """
        img_array = np.array(image)
        
        # Convert to grayscale for analysis
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        
        analysis = {
            'dimensions': {'width': image.width, 'height': image.height},
            'color_info': self._analyze_colors(img_array),
            'text_regions': self._detect_text_regions(gray),
            'layout': self._analyze_layout(gray),
            'timestamp': datetime.now().isoformat()
        }
        
        self.logger.info("‚úÖ Screen analysis complete")
        return analysis
    
    def _analyze_colors(self, img_array: np.ndarray) -> Dict[str, Any]:
        """Analyze color scheme and theme (Step 68)."""
        # Calculate average color
        avg_color = img_array.mean(axis=(0, 1))
        
        # Determine if dark or light theme
        brightness = avg_color.mean()
        theme = 'dark' if brightness < 128 else 'light'
        
        return {
            'average_color': avg_color.tolist(),
            'brightness': float(brightness),
            'theme': theme
        }
    
    def _detect_text_regions(self, gray: np.ndarray) -> List[Dict[str, Any]]:
        """Detect text regions using edge detection (Step 67)."""
        # Use edge detection to find text regions
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(
            edges,
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        text_regions = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # Filter small regions
            if w > 50 and h > 10:
                text_regions.append({
                    'x': int(x),
                    'y': int(y),
                    'width': int(w),
                    'height': int(h)
                })
        
        return text_regions[:50]  # Limit to top 50
    
    def _analyze_layout(self, gray: np.ndarray) -> Dict[str, Any]:
        """Analyze screen layout structure (Step 79)."""
        height, width = gray.shape
        
        # Divide screen into regions
        regions = {
            'top_bar': gray[0:int(height*0.1), :],
            'main_content': gray[int(height*0.1):int(height*0.9), :],
            'bottom_bar': gray[int(height*0.9):, :]
        }
        
        # Analyze each region
        layout = {}
        for region_name, region in regions.items():
            layout[region_name] = {
                'avg_brightness': float(region.mean()),
                'has_content': region.std() > 10  # High std = more content
            }
        
        return layout


# ============================================================================
# SECTION 5: Element Detection (Steps 91-120)
# ============================================================================

class ElementDetector:
    """
    UI element detection (Steps 91-120).
    
    Handles:
    - Button detection
    - Text field detection
    - Menu detection
    - Coordinate mapping
    """
    
    def __init__(self, config: VisionConfig, logger: VisionLogger):
        """Initialize element detector."""
        self.config = config
        self.logger = logger
    
    def detect_all_elements(self, image: Image.Image) -> List[Dict[str, Any]]:
        """
        Detect all UI elements (Steps 91-120).
        
        Args:
            image: PIL Image object
        
        Returns:
            List of detected elements with coordinates
        """
        img_array = np.array(image)
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        
        elements = []
        
        # Step 91-93: Detect buttons using edge detection
        elements.extend(self._detect_buttons(gray))
        
        # Step 94: Detect text fields
        elements.extend(self._detect_text_fields(gray))
        
        # Step 95-97: Detect interactive elements
        elements.extend(self._detect_interactive_elements(gray))
        
        self.logger.info(f"‚úÖ Detected {len(elements)} UI elements")
        return elements
    
    def _detect_buttons(self, gray: np.ndarray) -> List[Dict[str, Any]]:
        """Detect button-like rectangular regions (Steps 91-93)."""
        # Apply edge detection
        edges = cv2.Canny(
            gray,
            self.config.edge_detection_threshold,
            self.config.edge_detection_threshold * 2
        )
        
        # Find contours
        contours, _ = cv2.findContours(
            edges,
            cv2.RETR_TREE,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        buttons = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # Filter for button-like dimensions
            if (w > 50 and h > 20 and w < 300 and h < 100 and
                0.2 < h/w < 0.8):
                buttons.append({
                    'type': 'button',
                    'x': int(x),
                    'y': int(y),
                    'width': int(w),
                    'height': int(h),
                    'confidence': 0.7
                })
        
        return buttons[:20]  # Limit to top 20
    
    def _detect_text_fields(self, gray: np.ndarray) -> List[Dict[str, Any]]:
        """Detect text input fields (Step 94)."""
        # Text fields are usually horizontal rectangles
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(
            edges,
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        text_fields = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # Filter for text field dimensions (wide and short)
            if w > 100 and 20 < h < 50 and w/h > 3:
                text_fields.append({
                    'type': 'text_field',
                    'x': int(x),
                    'y': int(y),
                    'width': int(w),
                    'height': int(h),
                    'confidence': 0.6
                })
        
        return text_fields[:10]  # Limit to top 10
    
    def _detect_interactive_elements(self, gray: np.ndarray) -> List[Dict[str, Any]]:
        """Detect other interactive elements (Steps 95-97)."""
        # Simplified detection for checkboxes, radio buttons
        # In production, would use more sophisticated methods
        
        elements = []
        
        # Detect small square regions (checkboxes)
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(
            edges,
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # Small squares could be checkboxes
            if 10 < w < 30 and 10 < h < 30 and 0.8 < w/h < 1.2:
                elements.append({
                    'type': 'checkbox',
                    'x': int(x),
                    'y': int(y),
                    'width': int(w),
                    'height': int(h),
                    'confidence': 0.5
                })
        
        return elements[:15]  # Limit to top 15


# ============================================================================
# SECTION 6: Problem Detection (Steps 121-135)
# ============================================================================

class ProblemDetector:
    """
    Problem and issue detection (Steps 121-135).
    
    Handles:
    - Advertisement detection
    - Popup detection
    - Error message detection
    - UI freeze detection
    """
    
    def __init__(self, config: VisionConfig, logger: VisionLogger):
        """Initialize problem detector."""
        self.config = config
        self.logger = logger
    
    def detect_problems(
        self,
        image: Image.Image,
        gpt4_analysis: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Detect problems in screenshot (Steps 121-135).
        
        Args:
            image: PIL Image object
            gpt4_analysis: Optional GPT-4 Vision analysis result
        
        Returns:
            List of detected problems
        """
        problems = []
        
        # Use GPT-4 analysis if available
        if gpt4_analysis and 'problems' in gpt4_analysis:
            problems.extend([
                {'type': 'gpt4_detected', 'description': p}
                for p in gpt4_analysis['problems']
            ])
        
        # Additional heuristic detection
        img_array = np.array(image)
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        
        # Step 121-122: Detect popups (centered rectangles)
        popup_regions = self._detect_popups(gray)
        problems.extend([
            {'type': 'popup', 'region': r}
            for r in popup_regions
        ])
        
        # Step 123: Detect high-contrast regions (potential errors)
        error_regions = self._detect_high_contrast_regions(gray)
        if error_regions:
            problems.append({
                'type': 'potential_error',
                'count': len(error_regions)
            })
        
        self.logger.info(f"‚úÖ Detected {len(problems)} potential problems")
        return problems
    
    def _detect_popups(self, gray: np.ndarray) -> List[Dict[str, int]]:
        """Detect popup windows (Step 122)."""
        height, width = gray.shape
        center_x, center_y = width // 2, height // 2
        
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(
            edges,
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        popups = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # Check if centered and reasonably sized
            cx, cy = x + w//2, y + h//2
            distance_from_center = np.sqrt(
                (cx - center_x)**2 + (cy - center_y)**2
            )
            
            if (distance_from_center < min(width, height) * 0.2 and
                w > 200 and h > 100):
                popups.append({
                    'x': int(x),
                    'y': int(y),
                    'width': int(w),
                    'height': int(h)
                })
        
        return popups[:5]  # Limit to top 5
    
    def _detect_high_contrast_regions(self, gray: np.ndarray) -> List[Dict[str, int]]:
        """Detect high-contrast regions (potential errors) (Step 123)."""
        # Calculate local standard deviation
        kernel_size = 15
        mean = cv2.blur(gray, (kernel_size, kernel_size))
        mean_sq = cv2.blur(gray * gray, (kernel_size, kernel_size))
        std = np.sqrt(mean_sq - mean * mean)
        
        # Find high-variance regions
        high_contrast = std > 50
        contours, _ = cv2.findContours(
            high_contrast.astype(np.uint8),
            cv2.RETR_EXTERNAL,
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        regions = []
        for contour in contours:
            if cv2.contourArea(contour) > 1000:
                x, y, w, h = cv2.boundingRect(contour)
                regions.append({
                    'x': int(x),
                    'y': int(y),
                    'width': int(w),
                    'height': int(h)
                })
        
        return regions[:10]  # Limit to top 10


# ============================================================================
# SECTION 7: System Integration (Steps 136-150)
# ============================================================================

class VisionOrchestrator:
    """
    Main orchestrator for complete vision system (Steps 136-150).
    
    Coordinates all vision components and implements cognitive loop.
    """
    
    def __init__(self, api_key: Optional[str] = None, config: Optional[VisionConfig] = None):
        """
        Initialize vision orchestrator.
        
        Args:
            api_key: OpenAI API key
            config: Optional configuration
        """
        self.config = config or VisionConfig()
        
        # Initialize system (Steps 1-15)
        self.initializer = VisionInitializer(self.config)
        self.initializer.setup()
        
        # Get initialized components
        self.logger = self.initializer.logger
        self.cache = self.initializer.cache
        self.db = self.initializer.db
        
        # Initialize OpenAI client (Step 2)
        api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            self.logger.warning("‚ö†Ô∏è OpenAI API key not provided - GPT-4 Vision disabled")
            self.openai_client = None
        else:
            self.openai_client = openai.OpenAI(api_key=api_key)
            self.logger.info("‚úÖ OpenAI client configured")
        
        # Initialize components
        self.capture_manager = ScreenCaptureManager(
            self.config,
            self.logger,
            self.db
        )
        
        self.screen_analyzer = ScreenAnalyzer(
            self.config,
            self.logger
        )
        
        self.element_detector = ElementDetector(
            self.config,
            self.logger
        )
        
        self.problem_detector = ProblemDetector(
            self.config,
            self.logger
        )
        
        if self.openai_client:
            self.gpt4_analyzer = GPT4VisionAnalyzer(
                self.openai_client,
                self.config,
                self.logger,
                self.cache
            )
        else:
            self.gpt4_analyzer = None
    
    def run_complete_vision(self, save_screenshot: bool = True) -> Dict[str, Any]:
        """
        Run complete vision analysis pipeline (Steps 136-150).
        
        Args:
            save_screenshot: Whether to save screenshot to disk
        
        Returns:
            Complete analysis result
        """
        self.logger.info("üîç Starting complete vision analysis...")
        start_time = time.time()
        
        try:
            # Step 1: Capture screenshot
            screenshot, metadata = self.capture_manager.capture_screenshot()
            
            # Step 2: Save screenshot
            filepath = None
            if save_screenshot:
                filepath = self.capture_manager.save_screenshot(screenshot)
            
            # Step 3: Screen analysis
            screen_analysis = self.screen_analyzer.analyze_screen(screenshot)
            
            # Step 4: Element detection
            elements = self.element_detector.detect_all_elements(screenshot)
            
            # Step 5: GPT-4 Vision analysis (if available)
            gpt4_analysis = None
            if self.gpt4_analyzer:
                try:
                    gpt4_analysis = self.gpt4_analyzer.analyze_image(screenshot)
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è GPT-4 Vision failed: {e}")
            
            # Step 6: Problem detection
            problems = self.problem_detector.detect_problems(screenshot, gpt4_analysis)
            
            # Compile results
            result = {
                'success': True,
                'timestamp': datetime.now().isoformat(),
                'processing_time': time.time() - start_time,
                'screenshot': {
                    'filepath': filepath,
                    'metadata': metadata
                },
                'screen_analysis': screen_analysis,
                'elements': elements,
                'gpt4_analysis': gpt4_analysis,
                'problems': problems,
                'summary': {
                    'total_elements': len(elements),
                    'total_problems': len(problems),
                    'has_gpt4': gpt4_analysis is not None
                }
            }
            
            self.logger.info(
                f"‚úÖ Vision analysis complete! "
                f"Found {len(elements)} elements, "
                f"{len(problems)} problems "
                f"({result['processing_time']:.2f}s)"
            )
            
            return result
        
        except Exception as e:
            self.logger.error(f"‚ùå Vision analysis failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def cognitive_loop(
        self,
        iterations: int = 1,
        interval: float = 1.0
    ) -> List[Dict[str, Any]]:
        """
        Cognitive loop: see ‚Üí analyze ‚Üí decide ‚Üí act (Step 138).
        
        Args:
            iterations: Number of iterations to run
            interval: Seconds between iterations
        
        Returns:
            List of analysis results
        """
        self.logger.info(f"üîÑ Starting cognitive loop ({iterations} iterations)...")
        
        results = []
        for i in range(iterations):
            self.logger.info(f"üîÑ Iteration {i+1}/{iterations}")
            
            result = self.run_complete_vision(save_screenshot=True)
            results.append(result)
            
            if i < iterations - 1:
                time.sleep(interval)
        
        self.logger.info(f"‚úÖ Cognitive loop complete ({len(results)} iterations)")
        return results
    
    def cleanup(self) -> None:
        """Cleanup resources (Step 143)."""
        if self.db:
            self.db.close()
        if self.cache:
            self.cache.clear()
        self.logger.info("üßπ Cleanup complete")


# ============================================================================
# Public API / Initialization Functions
# ============================================================================

def initialize_vision_system(api_key: Optional[str] = None) -> VisionOrchestrator:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –∑—Ä–µ–Ω–∏—è MIRAI.
    
    Args:
        api_key: OpenAI API –∫–ª—é—á (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω)
    
    Returns:
        VisionOrchestrator: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑—Ä–µ–Ω–∏—è
    
    Example:
        >>> vision = initialize_vision_system()
        >>> result = vision.run_complete_vision()
        >>> print(result['summary'])
    """
    orchestrator = VisionOrchestrator()
    return orchestrator


# ============================================================================
# Main Entry Point
# ============================================================================

def main():
    """Main entry point for vision system."""
    print("üöÄ MIRAI Vision System v1.0.0")
    print("=" * 70)
    
    # Initialize orchestrator
    orchestrator = VisionOrchestrator()
    
    # Run single analysis
    result = orchestrator.run_complete_vision()
    
    # Print results
    if result['success']:
        print("\n‚úÖ Vision Analysis Results:")
        print(f"   ‚Ä¢ Elements detected: {result['summary']['total_elements']}")
        print(f"   ‚Ä¢ Problems found: {result['summary']['total_problems']}")
        print(f"   ‚Ä¢ GPT-4 available: {result['summary']['has_gpt4']}")
        print(f"   ‚Ä¢ Processing time: {result['processing_time']:.2f}s")
        
        if result['gpt4_analysis']:
            print(f"\nüìù GPT-4 Description:")
            print(f"   {result['gpt4_analysis'].get('description', 'N/A')}")
    else:
        print(f"\n‚ùå Analysis failed: {result.get('error', 'Unknown error')}")
    
    # Cleanup
    orchestrator.cleanup()


if __name__ == "__main__":
    main()
